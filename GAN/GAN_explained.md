#GAN explanation
<a href = "https://dreamgonfly.github.io/2018/03/17/gan-explained.html">link</a>

# Adversarial
GAN의 두번째 단어인 ‘Adversarial’은 GAN이 두 개의 모델을 적대적(Adversarial)으로 경쟁시키며 발전시킨다는 것을 뜻한다. 위조지폐범과 경찰을 생각해보자. 이 둘은 적대적인 경쟁 관계다. 위조지폐범은 경찰을 속이기 위해 점점 지폐 위조 제조 기술을 발전시키고, 경찰은 위조지폐범을 잡기 위해 점점 위폐를 찾는 기술을 발전시킨다. 시간이 흐르면 위조지폐범의 위폐 제조 기술은 완벽에 가깝게 발전할 것이다.

이처럼 GAN은 위조지폐범에 해당하는 생성자(Generator)와 경찰에 해당하는 구분자(Discriminator)를 경쟁적으로 학습시킨다. 생성자의 목적은 그럴듯한 가짜 데이터를 만들어서 구분자를 속이는 것이며, 구분자의 목적은 생성자가 만든 가짜 데이터와 진짜 데이터를 구분하는 것이다. 이 둘을 함께 학습시키면서 진짜와 구분할 수 없는 가짜를 만들어내는 생성자를 얻을 수 있다. 이것이 GAN의 핵심적인 아이디어인 적대적 학습(Adversarial Training)이다.

#Network
GAN의 마지막 단어 ‘네트워크(Network)’는 이 모델이 인공신경망(Artificial Neural Network) 또는 딥러닝(Deep Learning)으로 만들어졌기 때문에 붙었다. 이 글에서는 ‘인공신경망’과 ‘딥러닝’을 구분 없이 사용하겠다. 사실 적대적 학습이라는 개념을 구현하기 위해 반드시 딥러닝을 써야 하는 것은 아니다. 하지만 알파고 등 여러 사례에서 볼 수 있듯이 딥러닝은 강력한 머신러닝 모델을 가능하게 만드는 기술이다. 딥러닝이 이런 힘을 갖게 된 비결을 비선형 활성 함수(non-Linear Activation Function)와 계층(Hierarchy) 구조, 그리고 역전파(Backpropagation)

#Deep Learning
딥러닝의 강력함은 이런 층들이 매우 깊게 쌓을 수 있다는 점에서 나온다. 층을 많이 쌓을수록 더욱 복잡한 함수를 표현할 수 있는 힘(Representation Power)을 얻게 된다.

#Gradient Descent
딥러닝을 학습시킨다는 것은 최적의 가중치를 찾아간다는 것을 의미한다. 가중치는 처음에 랜덤으로 초기화되지만, 모델의 손실 함수(Loss Function)을 최소화시키는 방향으로 조금씩 업데이트된다. 이때 손실 함수 값이 역전파를 통해 각 층의 가중치에 전달되며 업데이트 방향과 크기를 결정한다. 이런 방식으로 가중치를 최적화하는 방식을 경사하강법(Gradient Descent)이라고 부른다.

#요약
요약하자면 GAN은 생성이라는 문제를 풀기 위해 딥러닝으로 만들어진 모델을 적대적 학습이라는 독특한 방식으로 학습시키는 알고리즘이다. 이처럼 ‘Generative Adversarial Network’라는 이름 속에는 모델의 목적부터 학습 방법까지 GAN의 전반적인 개념이 모두 들어있다.


#GAN의 전성시대를 연 DCGAN(Deep Convolutional GAN)
GAN은 학습이 불안정하기로 악명이 높다. 학습이 어렵다는 점은 GAN 모델이 다양한 곳에 응용되는 것을 가로막는 큰 장애물이었다. 이런 상황에서 수많은 실험 끝에 안정적인 학습이 가능한 GAN 모델의 구조를 찾아낸 것이 DCGAN이다.

DCGAN의 특징은 몇 가지로 요약할 수 있다. 먼저, 선형 레이어와 풀링 레이어(Pooling Layer)를 최대한 배제하고 합성곱(Convolution)과 ‘Transposed Convolution(Fractional-Strided Convolution)’으로 네트워크 구조를 만들었다. 풀링 레이어는 여러 딥러닝 모델에서 불필요한 매개변수의 수를 줄이고 중요한 특징만을 골라내는 역할을 하는 레이어지만 이미지의 위치 정보를 잃어버린다는 단점이 있다. 이미지를 생성하기 위해서는 위치 정보가 중요하기 때문에 DCGAN은 풀링 레이어를 배제했다. 선형 레이어 역시 마찬가지로 위치 정보를 잃어버리므로 모델의 깊은 레이어에서는 선형 레이어를 사용하지 않았다.

### Batch Normalization
DCGAN의 또 다른 특징은 배치 정규화(Batch Normalization)를 사용했다는 점이다. 배치 정규화는 레이어의 입력 데이터 분포가 치우쳐져 있을 때 평균과 분산을 조정해주는 역할을 한다. 이는 역전파가 각 레이어에 쉽게 전달되도록 해 학습이 안정적으로 이뤄지도록 돕는 데 중요한 역할을 한다.
